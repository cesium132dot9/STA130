# MCQ Practice Problems
Generated by NoteBookLM


## Week 7: Bootstrapping
### 1. What is the core idea behind the bootstrapping method as described in the sources? 
a) To divide the original sample into multiple subgroups for analysis. <br>
b) To perform hypothesis testing by assuming a null hypothesis. <br>
<span class="highlight-yellow">c) To estimate the sampling distribution of a statistic by resampling with replacement from the original sample. </span><br> 
d) To improve the accuracy of a point estimate by averaging it over different subsets of the data.

### 2. According to the sources, what does sampling with replacement mean in the context of bootstrapping? 
a) Each observation is selected from the original sample only once. <br> 
b) A smaller sample is drawn from the original sample without putting the selected observations back. <br> 
<span class="highlight-yellow"> c) After an observation is selected, it is put back into the original sample, allowing it to be selected again. </span><br>
d) Only a specific subset of the original data is used for resampling.

### 3. What is the primary purpose of generating a bootstrap sampling distribution? 
a) To estimate the true population parameter directly. <br> 
b) To create a new dataset with more variability than the original. <br>
<span class="highlight-yellow"> c) To estimate the variability of a sample statistic and construct confidence intervals. </span><br>
d) To test the assumptions of a statistical model.

### 4. According to the sources, how is a bootstrap confidence interval typically constructed? 
a) By assuming a specific theoretical distribution for the data. <br>
b) By adding and subtracting a fixed margin of error to the sample statistic. <br> 
<span class="highlight-yellow"> c) By finding the appropriate percentiles (e.g., 2.5th and 97.5th for a 95% CI) of the bootstrap sampling distribution. </span><br> 
d) By using the standard error calculated from the original sample and a critical value from a t-distribution. <br>


## Week 8: Linear Regression 
### 1. In a simple linear regression model ($y = \beta_0 + \beta_1x + \epsilon$), what does $\beta_1$ represent? 
a) The predicted value of the response variable when the predictor variable is zero. <br>
<span class="highlight-yellow"> b) The average change in the response variable for a one-unit increase in the predictor variable. </span><br>
c) The error term for the model. <br>
d) The overall fit of the regression line to the data. <br>

### 2. What is the difference between the true regression model and the fitted (or estimated) regression model according to the sources? 
a) The true model uses estimated coefficients, while the fitted model uses the actual population parameters. <br>
<span class="highlight-yellow"> b) The true model includes the error term ($\epsilon$) representing random deviations, while the fitted model ($\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x$) uses estimated coefficients. </span><br>
c) The fitted model is used for explanation, while the true model is used for prediction. <br>
d) There is no difference; they are two terms for the same concept. <br>

### 3. When a categorical predictor variable with two levels (e.g., 'new' and 'used') is included in a linear regression model, how is it typically handled as described in the sources? 
a) The categories are directly used as numerical values (e.g., 1 for 'new', 2 for 'used'). <br>
b) Separate regression models are fit for each category. <br>
<span class="highlight-yellow"> c) An indicator variable (also called a dummy variable) is created, which takes a value of 1 for one category and 0 for the baseline category. </span><br>
d) The categorical variable is ignored in the regression equation. <br>

### 4. What do residuals represent in the context of linear regression? 
a) The predicted values of the response variable. <br>
b) The coefficients of the predictor variables. <br>
<span class="highlight-yellow"> c) The difference between the observed values of the response variable and the values predicted by the regression model ($y_i - \hat{y}_i$). </span><br>
d) The unexplained variability in the predictor variables. <br>


## Week 9: Multiple Linear Regression 
### 1. In a multiple linear regression model, how is the interpretation of the coefficient for a particular predictor variable generally understood, assuming other predictors are held constant? 
a) It represents the total change in the response variable associated with that predictor. <br>
<span class="highlight-yellow"> b) It represents the average change in the response variable for a one-unit increase in that predictor, holding all other predictor variables constant. </span><br>
c) It indicates the significance of that predictor variable in the model. <br>
d) It reflects the correlation between that predictor and the response. <br>

### 2. According to the sources, what are common criteria used to compare and select between different linear regression models? 
a) Only the p-values of the coefficients. <br>
b) Only the simplicity of the model (number of predictors). <br>
<span class="highlight-yellow"> c) Measures of prediction accuracy like Root Mean Squared Error (RMSE) and the amount of variance explained like R-squared, while also considering model simplicity. </span><br> 
$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}i)^2} = \sqrt{\frac{1}{n} \sum{i=1}^{n} \epsilon_i^2}
$$ 
d) Primarily the size of the regression coefficients. 

### 3. When a categorical predictor with more than two levels is included in a multiple linear regression model, how is it typically handled as an extension of the binary case? 
a) One indicator variable is created for the entire categorical predictor. <br>
b) The levels are converted into a single numerical scale. <br>
<span class="highlight-yellow"> c) Multiple indicator variables are created, with one level chosen as the baseline, and each other level having its own indicator variable [Implied by extending the binary case]. </span><br>
d) Interaction terms with other predictors are automatically included for multi-level categorical predictors.


## Week 10: Classification Trees
### 1. What is the primary goal of supervised learning in the context of classification, as described in the sources?
a) To discover hidden patterns in unlabeled data. <br>
<span class="highlight-yellow"> b) To predict labels for new observations based on labeled historical data. </span><br>
c) To reduce the dimensionality of a dataset. <br>
d) To group data points into clusters without predefined categories.

### 2. What is the fundamental difference between supervised and unsupervised learning as described in the sources? 
a) Unsupervised learning requires more data than supervised learning. <br>
<span class="highlight-yellow"> b) Supervised learning uses labeled data to train a model to predict outcomes, while unsupervised learning finds patterns in unlabeled data. </span><br>
c) Supervised learning is used for categorical responses, while unsupervised learning is for numerical responses. <br>
d) Only supervised learning can be used for making predictions.

### 3. According to the sources, what is a key characteristic of the classification trees discussed? 
a) They are used for predicting numerical responses. <br>
b) They create continuous prediction surfaces. <br>
<span class="highlight-red"> c) They make predictions based on a series of binary (TRUE/FALSE) questions applied to the predictor variables, leading to categorical predictions. </span><br>
<span class="highlight-yellow"> d) They require all predictor variables to be categorical. </span>

### 4.What is the purpose of splitting nodes in a classification tree, according to the sources?
a) To increase the complexity of the tree as much as possible. <br>
b) To ensure that each terminal node contains an equal number of observations. <br>
<span class="highlight-yellow"> c) To create child nodes that are more "pure" with respect to the response variable, meaning they contain more observations from a single class. </span><br>
d) To reduce the number of predictor variables used in the tree.

### 5. What is a confusion matrix used for in the context of classification? 
a) To visualize the structure of a classification tree. <br>
b) To select the best predictor variables for a classification model. <br>
<span class="highlight-yellow"> c) To evaluate the performance of a classification model by showing the counts of true positives, true negatives, false positives, and false negatives. </span><br>
d) To determine the optimal splitting criteria in a classification tree.

### 6. According to the sources, how is sensitivity calculated in a binary classification problem? 
a) $\frac{\text{True Negatives}}{\text{Total Actual Negatives}}$ <br>
b) $\frac{\text{False Positives}}{\text{Total Predictions}}$ <br>
<span class="highlight-red"> c) $\frac{\text{True Positives}}{\text{Total Actual Positives}}$ </span><br>
d) $\frac{\text{True Negatives}}{\text{Total Predictions}}$ 


## Week 11: Predictive Performance of Models, Ensemble Methods
### 1. What is the primary purpose of evaluating the out-of-sample predictive performance of a model? 
a) To assess how well the model fits the data it was trained on. <br>
b) To calculate the p-values for the model coefficients. <br>
<span class="highlight-yellow"> c) To estimate how accurately the model will predict outcomes for new, unseen data. </span><br>
d) To simplify the model and make it more interpretable.

### 2. According to the sources, how can we estimate the out-of-sample performance when we don't have completely new data? 
a) By increasing the complexity of the model. <br>
b) By using all the available data to train the model for the best possible fit. <br>
<span class="highlight-yellow"> c) By splitting the existing data into a training set (to build the model) and a testing set (to evaluate its performance). </span><br>
d) By using techniques like cross-validation on the training data alone.

### 3. What is the main idea behind ensemble methods for prediction? 
a) To select the single best model from a collection of models. <br>
<span class="highlight-yellow"> b) To combine the predictions of multiple individual "base" models to improve overall predictive accuracy and robustness. </span><br>
c) To reduce the number of predictor variables in a single complex model. <br>
d) To focus on creating more interpretable prediction models.

### 4. What is Bootstrap Aggregating (Bagging) as described in the sources? 
a) A method for selecting a subset of the most important predictor variables. <br>
b) A technique for improving the splitting criteria in a single decision tree. <br>
<span class="highlight-yellow"> c) An ensemble method that involves creating multiple bootstrap samples from the training data, fitting a base model (often a decision tree) on each sample, and then aggregating their predictions (e.g., by majority vote for classification). </span><br>
d) A way to sequentially build a single strong model by correcting the errors of previous models.

### 5. How do Random Forests differ from Bagging, according to the sources? 
a) Random Forests use a different resampling technique than Bagging. <br>
b) Bagging can be used for regression, while Random Forests are only for classification. <br>
<span class="highlight-yellow"> c) In addition to using bootstrap samples, Random Forests also randomly select a subset of predictor variables to consider at each split in the individual trees, thus increasing diversity among the trees. </span><br>
d) Bagging trains trees sequentially, while Random Forests train them in parallel.


## Week 12: Motivation for Advanced Statistical Techniques 
### 1. What is Simpson's Paradox, as illustrated by the UC Berkeley admissions example? 
a) A statistical result that is significant but has no practical importance. <br>
<span class="highlight-red"> b) A situation where a trend is observed in individual groups but disappears or reverses when the groups are combined. </span><br>
c) The difficulty in interpreting statistical results when there is a large amount of missing data. <br>
d) A bias that occurs when the sample is not representative of the population.

### 2. According to the sources, what does Missing Completely at Random (MCAR) mean? 
a) The missingness depends on the value of the variable that is missing. <br>
b) The missingness depends on the value of other observed variables. <br>
<span class="highlight-red"> c) The probability of a value being missing is the same for all observations and is unrelated to any observed or unobserved variables. </span><br>
d) The missingness is systematic and follows a specific pattern.

### 3. Based on the sources, what is the prior probability ($P(A)$) in Bayes' Rule? 
a) The probability of the data given the hypothesis. <br>
b) The probability of the data and the hypothesis occurring together. <br>
<span class="highlight-yellow"> c) The initial belief or probability of a hypothesis before observing any data. </span><br>
d) The updated probability of the hypothesis after observing the data.

### 4. What is the key distinction between using a statistical model for explanation versus using it for prediction, according to the sources? 
a) Explanatory models prioritize out-of-sample accuracy, while predictive models focus on in-sample fit. <br>
b) Predictive models use p-values to assess variable significance, while explanatory models do not. <br>
<span class="highlight-yellow"> c) Explanatory models aim to understand the underlying relationships and potential causal links between variables, while predictive models focus on accurately forecasting future outcomes. </span><br>
d) Explanatory models always involve simpler model structures than predictive models. 